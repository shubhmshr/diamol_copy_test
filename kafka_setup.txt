# ssh into box
ssh -i ~/.aws/HadoopCluster.pem ec2-user@<ec2-instance>


# Install Java 8
wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.rpm
sudo yum install -y jdk-8u141-linux-x64.rpm
java -version


# Install Kafka
1. wget http://mirrors.advancedhosters.com/apache/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz
2. tar -xzf kafka_2.11-0.10.2.1.tgz
3. rm kafka_2.11-0.10.2.1.tgz

# Zookeeper Properties
1. sudo mkdir -p /var/zookeeper/data
2. sudo chown -R ec2-user /var/zookeeper
3. vi kafka_2.11-0.10.2.1/config/zookeeper.properties

dataDir=/var/zookeeper/data
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0

server.1=<ec2-instance-1>:2888:3888
server.2=<ec2-instance-2>:2888:3888
# Add more servers if you want to
initLimit=5
syncLimit=2

4. On <ec2-instance-1> run echo "1" > /var/zookeeper/data/myid
5. On <ec2-instance-2> run echo "2" > /var/zookeeper/data/myid
6. Run above respective echo "no" cmd on all ec2-instances


# Start Kafka Server
1. cd kafka_2.11-0.10.2.1/
2. sudo mkdir -p /var/kafka/data
3. sudo chown -R ec2-user /var/kafka
2. change following properties in config/server.properties:
        # Change broker id in multi node cluster
        broker.id={id}

        # Change Log Dir
        log.dirs=/var/kafka/data

        # Change zookeeper connect string
        zookeeper.connect=<ec2-instance-1>:2181,<ec2-instance-2>:2181

        # to enable communication change listeners to public dns name in each ec2 instance respectively
        listeners=PLAINTEXT://<ec2-instance-1>:9092

        # Change the following property to allow kafka to produce timestamp
        log.message.timestamp.type=LogAppendTime

        # log retention hours to save disk space
        log.retention.hours=1

        # log retention check interval
        log.retention.check.interval.ms=300000

        # How much data you want to retain
        log.retention.bytes=134217728

        # Each log segment size
	log.segment.bytes=134217728

        # Maximum message size broker/server will accept
        message.max.bytes=15728640


# Clean up existing dirs
ps -ef | grep kafka | awk '{print $2}' | xargs kill -9
rm -rf /var/kafka/data/*
rm -rf /var/zookeeper/data/*
echo "1" > /var/zookeeper/data/myid
echo "2" > /var/zookeeper/data/myid


# Start Zookeeper
nohup bin/zookeeper-server-start.sh config/zookeeper.properties > ~/zookeeper-logs 2>&1 &


# Start Kafka server
change KAFKA_HEAP_OPTS="-Xmx3G -Xms3G" in bin/kafka-server-start.sh # To increase Java Heap Space
nohup bin/kafka-server-start.sh config/server.properties > ~/kafka-logs 2>&1 &


# Check list of all available brokers
bin/zookeeper-shell.sh localhost:2181 <<< "ls /brokers/ids"


# Create Kafka Topic
bin/kafka-topics.sh --zookeeper <ec2-instance-1>:2181,<ec2-instance-2>:2181 --create --topic maxwell --partitions 10 --replication-factor 2


# Describe Topic - Partition's Leaders should be distributed.
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic maxwell


# Delete Kafka Topic
bin/kafka-configs.sh --zookeeper localhost --alter --entity-type topics --entity-name maxwell --add-config delete.topic.enable=true
bin/kafka-topics.sh --zookeeper <ec2-instance-1>:2181,<ec2-instance-2>:2181 --delete --topic maxwell
bin/zookeeper-shell.sh localhost:2181 rmr /config/topics/maxwell
bin/zookeeper-shell.sh localhost:2181 rmr /brokers/topics/maxwell
bin/zookeeper-shell.sh localhost:2181 rmr /admin/delete_topics/maxwell


# To change retention for topic as well:
bin/kafka-configs.sh --zookeeper localhost --alter --entity-type topics --entity-name maxwell --add-config retention.ms=1800000

bin/kafka-configs.sh --zookeeper localhost --alter --entity-type topics --entity-name maxwell --add-config message.timestamp.type=LogAppendTime

bin/kafka-configs.sh --zookeeper localhost --alter --entity-type topics --entity-name maxwell --add-config delete.topic.enable=true

# List Topics
bin/kafka-topics.sh --zookeeper <ec2-instance-1>:2181,<ec2-instance-2>:2181 --list


# Start CLI Producer
bin/kafka-console-producer.sh --broker-list <ec2-instance-1>:9092,<ec2-instance-2>:9092 --topic test


# Start CLI Consumer
bin/kafka-console-consumer.sh --bootstrap-server <ec2-instance-1>:9092,<ec2-instance-2>:9092 --topic test --from-beginning


# Get latest offsets
bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list <ec2-instance-1>:9092,<ec2-instance-2>:9092 --topic maxwell


# Kafka CLI read from offset
bin/kafka-simple-consumer-shell.sh --broker-list <ec2-instance-1>:9092,<ec2-instance-2>:9092 --topic maxwell --partition 9 --offset 13289626


#Lightweight Kafka tool
https://github.com/fgeller/kt

kt consume -brokers <ec2-instance-1>:9092,<ec2-instance-2>:9092 -topic maxwell -offsets 9=13291854:13291855

kafka-console-consumer.sh --bootstrap-server 10.104.0.10:9092,10.104.0.11:9092,10.104.0.12:9092 --topic maxwell --offset 431834503 --partition 0 --max-messages 1

Useful Resource: https://aws.amazon.com/blogs/big-data/real-time-stream-processing-using-apache-spark-streaming-and-apache-kafka-on-aws/
